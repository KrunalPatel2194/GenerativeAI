{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "648e1a3c-3a1c-44cb-b2d8-350bb43e66c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./myenv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in ./myenv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: peft in ./myenv/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: evaluate in ./myenv/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in ./myenv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in ./myenv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: accelerate in ./myenv/lib/python3.12/site-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./myenv/lib/python3.12/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./myenv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./myenv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./myenv/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./myenv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./myenv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./myenv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./myenv/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./myenv/lib/python3.12/site-packages (from peft) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in ./myenv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./myenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nidhipatel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries (run this cell once)\n",
    "!pip install transformers datasets peft evaluate scikit-learn nltk accelerate\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModelForSequenceClassification\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7238f35b-3674-47df-9a6b-285f470a682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Sentence: quick brown fox jumps lazy dog\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Text Preprocessing Enhancements\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the text by lowercasing, removing punctuation, and filtering stopwords.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text.split()\n",
    "    filtered_tokens = [word for word in text_tokens if word not in stop_words]\n",
    "\n",
    "    # Rejoin tokens into a single string\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Sample sentence to demonstrate preprocessing\n",
    "sample_sentence = \"The quick brown fox jumps over the lazy dog!\"\n",
    "processed_sentence = preprocess_text(sample_sentence)\n",
    "print(f\"Processed Sentence: {processed_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "503e1205-41bf-4c0b-849d-514f6df94ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Dataset and Apply Preprocessing\n",
    "dataset = load_dataset(\"emotion\")\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    return [preprocess_text(sentence) for sentence in dataset['text']]\n",
    "\n",
    "# Preprocess the training and test sets\n",
    "dataset = dataset.map(lambda x: {'text': preprocess_dataset(x)}, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21a308b8-a99f-4a78-8730-1fecedd65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 6767.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52fb2588-81fb-4e3f-b12c-311284c2fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load Pre-Trained Model\n",
    "device = torch.device(\"cpu\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6).to(device)\n",
    "\n",
    "# Step 5: Define Metrics (F1, Precision, Recall, and Accuracy)\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86603630-46bb-4608-ab2f-07a088ed0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the base model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Results: {'eval_loss': 1.7597534656524658, 'eval_model_preparation_time': 0.0014, 'eval_accuracy': 0.145, 'eval_f1': 0.08822817790373491, 'eval_precision': 0.23849116796695127, 'eval_recall': 0.145, 'eval_runtime': 93.0177, 'eval_samples_per_second': 21.501, 'eval_steps_per_second': 1.344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the Base Model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_base_model\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=False,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate the base model before fine-tuning\n",
    "print(\"Evaluating the base model...\")\n",
    "base_results = trainer.evaluate()\n",
    "print(f\"Base Model Results: {base_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5503ed25-da98-4de6-9aae-43276642794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Fine-Tune Using PEFT (LoRA)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]  # Targeting attention layers in DistilBERT\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Select a smaller subset for quick fine-tuning\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5cb20a9-9e8f-44b8-b236-8dce587b7604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 06:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.690976</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.219639</td>\n",
       "      <td>0.223004</td>\n",
       "      <td>0.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.657593</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.213380</td>\n",
       "      <td>0.222593</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.648153</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.212086</td>\n",
       "      <td>0.223542</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nidhipatel/Documents/DentaFlex/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=1.7034879294774865, metrics={'train_runtime': 406.4393, 'train_samples_per_second': 3.691, 'train_steps_per_second': 0.229, 'total_flos': 200520359018496.0, 'train_loss': 1.7034879294774865, 'epoch': 2.976})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Fine-Tuning\n",
    "fine_tune_args = TrainingArguments(\n",
    "    output_dir=\"./results_peft_model\",\n",
    "    per_device_train_batch_size=4,  # Reduce batch size to fit in memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Use gradient accumulation to simulate a larger batch size\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs_peft\"\n",
    ")\n",
    "\n",
    "# Trainer for fine-tuning\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=fine_tune_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3c322-2a27-4608-b710-a24032cebb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='416' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [416/500 01:22 < 00:16, 5.02 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 9: Evaluate the Fine-Tuned Model\n",
    "print(\"Evaluating the fine-tuned model...\")\n",
    "peft_results = peft_trainer.evaluate()\n",
    "print(f\"Fine-Tuned Model Results: {peft_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb7574-4fbf-4832-9403-19142ec4ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the Fine-Tuned Model\n",
    "save_directory = \"./peft_fine_tuned_model\"\n",
    "peft_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4f008-0f43-4290-9ac2-cfb0e57912ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# Step 11: Load the Fine-Tuned Model using PeftModelForSequenceClassification\n",
    "# Load the PEFT model on top of the base model\n",
    "loaded_model = PeftModel.from_pretrained(base_model, save_directory).to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Preprocess and perform inference\n",
    "def perform_inference(sentence: str):\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    inputs = loaded_tokenizer(preprocessed_sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.item()\n",
    "\n",
    "# Example inference\n",
    "sentence = \"I love programming!\"\n",
    "predicted_label = perform_inference(sentence)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa46eb-fda8-465b-ba84-4ddc80d70a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Compare Base and Fine-Tuned Model Results\n",
    "print(\"\\nComparison of Base and Fine-Tuned Model:\")\n",
    "print(f\"Base Model Accuracy: {base_results['eval_accuracy']}\")\n",
    "print(f\"Fine-Tuned Model Accuracy: {peft_results['eval_accuracy']}\")\n",
    "print(f\"Base Model F1 Score: {base_results.get('eval_f1', 'N/A')}\")\n",
    "print(f\"Fine-Tuned Model F1 Score: {peft_results.get('eval_f1', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04f032-9d7c-426a-97fc-fd65c8fbb4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
